{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Please follow the setup instructions in [DEVELOPER.md](../../DEVELOPER.md) before running this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "import numpy as np\n",
        "import pyarrow as pa\n",
        "import uuid\n",
        "\n",
        "from blob_access_basic import upload_blob, download_blob_as_file, download_blob_with_pandas\n",
        "from data_access import list_objects, upload_object, download_object_by_name\n",
        "from notebook_helpers import format_objects\n",
        "import tools.example_instances"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Listing objects\n",
        "- [GET https://<host>/<org_id>/objects?path=<path>]()\n",
        "- returns a list of items:\n",
        "  - folders have a `name` and in `links.list` a URL to list that folder\n",
        "  - files have a `name`, `schema`, `version`, and in `links.download` a URL to download that object\n",
        "  - filtering, pagination, history: TBD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "format_objects(list_objects, path='/')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Uploading objects\n",
        "\n",
        "### 1. Upload blobs to Azure\n",
        "- write tables to Parquet files and calculate SHA-256 hashes\n",
        "  - if the table is generated on-the-fly (streamed), use a UUID instead of the hash. This breaks automatic deduplication.\n",
        "- request upload URLs: [PUT https://<host>/<org_id>/data]() with the list of hashes/UUIDs\n",
        "- for each hash/UUID this returns an `exists` flag, and if it does not exist, an upload URL\n",
        "- upload the Parquet files to the upload-urls (Azure blob storage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "imported_points = np.loadtxt(\"d:\\\\Topo.csv\", skiprows=1, delimiter=',')\n",
        "schema = pa.schema([(\"x\", pa.float64()), (\"y\", pa.float64()), (\"z\", pa.float64())])\n",
        "\n",
        "upload_result = upload_blob(imported_points, schema)\n",
        "upload_result"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 2. Build the object\n",
        "- follow the schema to build a JSON object\n",
        "- add the hashes/uuids from the previous step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "bbox_min = np.min(imported_points, axis=0)\n",
        "bbox_max = np.max(imported_points, axis=0)\n",
        "point_set =  {\n",
        "  \"$schema\": \"/objects/pointset/1.0.1/pointset.schema.json\",\n",
        "  \"id\": str(uuid.uuid4()),\n",
        "  \"name\": \"Topography\",\n",
        "  \"description\": \"Our imported topography points\",\n",
        "  \"locations\": {\n",
        "    \"coordinates\": {\n",
        "      \"width\": 3,\n",
        "      \"data_type\": \"float64\",\n",
        "      \"length\": len(imported_points),\n",
        "      \"data\": upload_result.hash\n",
        "    }\n",
        "  },\n",
        "  \"bounding_box\": {\n",
        "    \"min_x\": bbox_min[0],\n",
        "    \"max_x\": bbox_max[0],\n",
        "    \"min_y\": bbox_min[1],\n",
        "    \"max_y\": bbox_max[1],\n",
        "    \"min_z\": bbox_min[2],\n",
        "    \"max_z\": bbox_max[2],\n",
        "  },\n",
        "  \"coordinate_reference_system\": {\n",
        "    \"epsg_code\": 2048\n",
        "  },\n",
        "  \"attributes\": []\n",
        "}\n",
        "point_set_json = json.dumps(point_set, indent=4)\n",
        "print(point_set_json)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3. Optional: validate the object\n",
        "- we use [jsonschema](https://pypi.org/project/jsonschema/) for validation\n",
        "- because of the use of composition, validation errors are usually not very helpful"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "schema = tools.example_instances.example_schema_dict(point_set)\n",
        "tools.example_instances.validate_data(point_set, schema)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 4. Upload the object to the Geoscience Object API\n",
        "- [POST https://<host>/<org_id>/objects/<path>/<name>]()\n",
        "- returns the object, version, and download URL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "upload_object(point_set_json, name=\"pointset_1.json\", path='/examples/pointsets')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Downloading objects\n",
        "\n",
        "### 1. Download the object from the Geoscience Object API\n",
        "- use the download URL returned by the List Objects request or build it from a known path+name\n",
        "- [GET https://<host>/<org_id>/objects/<path>/<name>]()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "downloaded_object = download_object_by_name(name=\"pointset_1.json\", path='/examples/pointsets')\n",
        "coordinates_hash = downloaded_object.locations.coordinates.data\n",
        "downloaded_object"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###  2. Download blobs from Azure\n",
        "- collect all blob-hashes/UUIDs from the object\n",
        "- request download URLs: [POST https://<host>/<org_id>/data]() with the list of hashes/UUIDs\n",
        "- for each hash/UUID this returns an 'exists' flag and if it exists, a download URL\n",
        "- download the blob"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# with pandas\n",
        "download_blob_with_pandas(coordinates_hash)\n",
        "\n",
        "# with azure-storage-blob\n",
        "download_blob_as_file(coordinates_hash)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
